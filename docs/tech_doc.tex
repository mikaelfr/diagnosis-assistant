\documentclass{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}

\title{Pre-hospital Diagnosis Assistant - Technical Documentation}
\author{Mikael Fredriksson, Simo M채kel채, Karine Heinonen}
\date{October 2021}

\begin{document}
\maketitle

\section{Data generation}
Since the medical data of patients is a closely guarded secret, there was obviously no way for us to be granted access to this data. Finding synthetic data was also very difficult, especially in 2021 when most medical datasets were all about COVID. While we did eventually manage to find a dataset of a lot of anonymized EMS data, whether that contained any useful data was a mystery. Not to mention getting the data required going through an application process and after that getting 160gb of data mailed to you on a USB stick. Slightly beyond the scope of this mini-project.

Thus the only option left was to generate our own data. Luckily Simo M채kel채 is very knowledgeable in the field and was able to spec what needed to be done for the rest of the team.

The first method we tried, was to generate all the vital signs first using a normal distribution and then assigning diagnoses to suitable deviations. This turned out to not work too well. The distributions between diagnoses was pretty bad, some diagnoses were abundant and there were barely any (sometimes none) rows for other diagnoses. This method is called \texttt{generate\_people} in the code.

The second method we tried, and are still using, was to generate healthy people and each diagnosis separately. We take a healthy person and push the values to wanted directions to achieve the symptoms of a specified diagnosis. We also add some randomness, like missing values and diagnoses slightly differing from the actual values occasionally. This method is called \texttt{generate\_people2} in the code.

All data generation code can be found in a file called \texttt{patient\_generator.py}.

\section{Building the model}
We originally just used a default \texttt{RandomForestClassifier} from \texttt{scikit}. This worked fine and gave us an accuracy of around 85\%. The mean confidence for wrong classification was around 60\% and for correct ones around 90\%. Since we show in the UI a list of all diagnoses and their probabilities, seeing 60\% confidence tells the user to not trust this particular prediction too much.

We ran into a problem when trying to optimize the model with TPOT. In the first iteration we used \texttt{generate\_people} to generate a list of patients and that list was randomized by default. When we switched to using \texttt{generate\_people2} all of the healthy people were in the train set and pretty much only people with diagnoses were in the test set. Thus we overfit the data pretty hard for the training set and the test set accuracy was hovering a bit under 90\%. Not bad, but way lower than what TPOT was claiming to have gotten for the training set. Shuffling the data set before training was a simple solution for this issue.

The code for running the classifier pipeline can be found in a file called \texttt{pipeline.py} and the actual classification code and some data cleanup + preparation can be found in a file called \texttt{classifier.py}.

\section{Visualization}
Very early on Karine Heinonen found a service called \texttt{Streamlit} which turned out to be excellent for our purposes. That allowed us to very easily deploy and display visualization straight from python code. It also allowed great interactivity with users and ourselves as well. It was much easier to just update the pages in the Streamlit app for debugging purposes. It was invaluable for debugging both generation functions quickly. 

The Streamlit app can be found in \texttt{streamlit\_app.py}.
\end{document}